{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the path to the evaluation results for each direction in the following variables:\n",
    "- `eval_results_path`\n",
    "- `reverse_eval_results_path`\n",
    "\n",
    "\"A\" and \"B\" will be based on \"A\" and \"B\" from `eval_results_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_path = \"\"\n",
    "reverse_eval_results_path = \"\"\n",
    "\n",
    "forward_map = {\n",
    "    \"A\": \"A\",\n",
    "    \"B\": \"B\",\n",
    "    \"tie\": \"tie\",\n",
    "}\n",
    "\n",
    "reverse_map = {\n",
    "    \"B\": \"A\",\n",
    "    \"A\": \"B\",\n",
    "    \"tie\": \"tie\",\n",
    "}\n",
    "\n",
    "with open(eval_results_path) as f:\n",
    "    eval_results = json.load(f)\n",
    "\n",
    "with open(reverse_eval_results_path) as f:\n",
    "    reverse_eval_results = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are output after the cell and the bidirectional merged results are saved in the same directory as `eval_results_path` with the name `robust_merged` added to the original name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [\"contrast\", \"relevancy\", \"diversity\", \"usefulness\"]\n",
    "baseline = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 0,\n",
    "    \"tie\": 0,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "overall_results = tuple(zip(eval_results, reverse_eval_results))\n",
    "\n",
    "merged_results = {}\n",
    "\n",
    "for forward, backward in overall_results:\n",
    "    asp_results = forward[\"aspect_eval\"]\n",
    "    reverse_asp_results = backward[\"aspect_eval\"]\n",
    "    counts = {criterion: baseline.copy() for criterion in criteria}\n",
    "    counts[\"query\"] = forward[\"query\"]\n",
    "    assert counts[\"query\"] == backward[\"query\"]\n",
    "\n",
    "    if counts[\"query\"] not in merged_results:\n",
    "        merged_results[counts[\"query\"]] = {}\n",
    "\n",
    "    curr_wins_A = {criterion: 0 for criterion in criteria}\n",
    "    curr_wins_B = {criterion: 0 for criterion in criteria}\n",
    "    for asp_result in asp_results:\n",
    "        if asp_result not in reverse_asp_results:\n",
    "            print(asp_result)\n",
    "            continue\n",
    "        if asp_result not in merged_results[counts[\"query\"]]:\n",
    "            merged_results[counts[\"query\"]][asp_result] = {}\n",
    "\n",
    "        for criterion in criteria:\n",
    "            asp_results[asp_result][criterion] = asp_results[asp_result][criterion].lower() if len(asp_results[asp_result][criterion]) > 1 else asp_results[asp_result][criterion]\n",
    "            reverse_asp_results[asp_result][criterion] = reverse_asp_results[asp_result][criterion].lower() if len(reverse_asp_results[asp_result][criterion]) > 1 else reverse_asp_results[asp_result][criterion]\n",
    "\n",
    "            winner = forward_map[asp_results[asp_result][criterion]]\n",
    "            reverse_winner = reverse_map[reverse_asp_results[asp_result][criterion]]\n",
    "\n",
    "            if winner == reverse_winner:\n",
    "                counts[criterion][winner] += 1\n",
    "                curr_wins_A[criterion] += 1 if winner == \"A\" else 0\n",
    "                curr_wins_B[criterion] += 1 if winner == \"B\" else 0\n",
    "                merged_results[counts[\"query\"]][asp_result][criterion] = winner\n",
    "            else:\n",
    "                counts[criterion][\"tie\"] += 1\n",
    "                merged_results[counts[\"query\"]][asp_result][criterion] = \"tie\"\n",
    "\n",
    "    results.append(counts)\n",
    "curr = \"pairwise\"\n",
    "assert curr in eval_results_path\n",
    "merged_path = eval_results_path.replace(curr, \"robust_merged_pairwise\")\n",
    "with open(merged_path, \"w\") as f:\n",
    "    json.dump(merged_results, f)\n",
    "\n",
    "# aggregate A, B tier results for each criterion\n",
    "aggregated_results = {criterion: baseline.copy() for criterion in criteria}\n",
    "for result in results:\n",
    "    for criterion in criteria:\n",
    "        for tier in [\"A\", \"B\", \"tie\"]:\n",
    "            aggregated_results[criterion][tier] += result[criterion][tier]\n",
    "\n",
    "\n",
    "for criterion in criteria:\n",
    "    tracker = []\n",
    "    A = aggregated_results[criterion][\"A\"]\n",
    "    B = aggregated_results[criterion][\"B\"]\n",
    "    tie = aggregated_results[criterion][\"tie\"]\n",
    "    tracker.extend([1] * A)\n",
    "    tracker.extend([0] * B)\n",
    "    tracker.extend([0.5] * tie)\n",
    "    print(f\"Criterion: {criterion}\")\n",
    "    t, p = stats.ttest_1samp(tracker, 0.5)\n",
    "    A_win_rate = sum(tracker) / (A + B + tie)\n",
    "    print(f\"t: {t}, p: {p}, A win rate: {A_win_rate*100}%\")\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "print(\"\\nResults for each criterions, 95% CI in brackets\")\n",
    "for criterion in criteria:\n",
    "    tracker = []\n",
    "    A = aggregated_results[criterion][\"A\"]\n",
    "    B = aggregated_results[criterion][\"B\"]\n",
    "    tie = aggregated_results[criterion][\"tie\"]\n",
    "    tracker.extend([1] * A)\n",
    "    tracker.extend([0] * B)\n",
    "    tracker.extend([0.5] * tie)\n",
    "    \n",
    "    # Calculate 95% CI\n",
    "    mean, ci_lower, ci_upper = mean_confidence_interval(tracker)\n",
    "    # print(f\"95% CI for {criterion}: ({ci_lower:.2f}, {ci_upper:.2f})\")\n",
    "    print(f\"{criterion}:\\n{mean:.2f} [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
